# üöÄ Transforma√ß√£o de Dados com Databricks!
![1](https://github.com/user-attachments/assets/06e48b13-f11a-455e-9ae8-0db518b474ca)

![2](https://github.com/user-attachments/assets/eb464b8e-f1be-4e8b-af0c-2001a2336984)


### Este √©  um projeto de convers√£o de arquivos CSV para Parquet no Databricks, um processo que visou otimizar o armazenamento e melhorar a performance de leitura de grandes volumes de dados. Para isso, utilizei a plataforma Databricks para criar e executar um job tornando o processo mais eficiente e escal√°vel.

### üîß O que foi feito:

### Importa√ß√£o de arquivos CSV
Convers√£o para o formato Parquet, que oferece compress√£o e leitura mais r√°pidas
Cria√ß√£o de um job no Databricks para execu√ß√£o automatizada
üí° Por que Parquet? O formato Parquet √© ideal para ambientes de Big Data, pois:

### Reduz o espa√ßo de armazenamento
Aumenta a performance nas consultas
Permite uma compress√£o eficiente de dados
üíª Ferramentas utilizadas:

### Databricks (para executar o job)
Python e Spark (para manipula√ß√£o dos dados)

![screencapture-community-cloud-databricks-editor-notebooks-4189562596680307-2025-01-04-15_57_10](https://github.com/user-attachments/assets/8fb7c634-c986-4424-8318-8aa191b56278)



![FireShot Capture 002 - Csv - Parquet - Databricks Community Edition - community cloud databricks com](https://github.com/user-attachments/assets/b0cc6895-9f44-4c0b-ad00-0da8a9f6677b)
