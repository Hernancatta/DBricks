# ğŸš€ TransformaÃ§Ã£o de Dados com Databricks!
![1](https://github.com/user-attachments/assets/06e48b13-f11a-455e-9ae8-0db518b474ca)

![2](https://github.com/user-attachments/assets/eb464b8e-f1be-4e8b-af0c-2001a2336984)


### Este Ã©  um projeto de conversÃ£o de arquivos CSV para Parquet no Databricks, um processo que visou otimizar o armazenamento e melhorar a performance de leitura de grandes volumes de dados. Para isso, utilizei a plataforma Databricks para criar e executar um job que automatizou a transformaÃ§Ã£o, tornando o processo mais eficiente e escalÃ¡vel.

### ğŸ”§ O que foi feito:

### ImportaÃ§Ã£o de arquivos CSV
ConversÃ£o para o formato Parquet, que oferece compressÃ£o e leitura mais rÃ¡pidas
CriaÃ§Ã£o de um job no Databricks para execuÃ§Ã£o automatizada
ğŸ’¡ Por que Parquet? O formato Parquet Ã© ideal para ambientes de Big Data, pois:

### Reduz o espaÃ§o de armazenamento
Aumenta a performance nas consultas
Permite uma compressÃ£o eficiente de dados
ğŸ’» Ferramentas utilizadas:

### Databricks (para executar o job)
Python e Spark (para manipulaÃ§Ã£o dos dados)

![screencapture-community-cloud-databricks-editor-notebooks-4189562596680307-2025-01-04-15_57_10](https://github.com/user-attachments/assets/8fb7c634-c986-4424-8318-8aa191b56278)
